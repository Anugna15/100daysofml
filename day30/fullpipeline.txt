Full Preprocessing + Modeling Pipeline on Iris Dataset

## 🧠 Objective:
Build a complete ML pipeline that includes:
- Feature scaling using StandardScaler
- Modeling using RandomForestClassifier
- Evaluation with classification report and cross-validation
- Visualization of feature importances

---

## 📊 Dataset:
- **Name**: Iris Dataset
- **Source**: sklearn.datasets.load_iris
- **Features**: 4 numerical features (sepal length, sepal width, petal length, petal width)
- **Target**: Species of Iris flower (setosa, versicolor, virginica)

---

## 🧱 Steps Implemented:

1. **Train/Test Split**
   - Used `train_test_split` with 80-20 split.

2. **Preprocessing Pipeline**
   - Created a `ColumnTransformer` to apply `StandardScaler` on all features.
   - Passed the preprocessor into a `Pipeline` with a `RandomForestClassifier`.

3. **Model Training**
   - Fit the pipeline to the training data using `pipeline.fit()`.

4. **Prediction & Evaluation**
   - Predicted values using `pipeline.predict()`.
   - Evaluated using `accuracy_score` and `classification_report`.

5. **Cross-Validation**
   - Used `StratifiedKFold` with 5 splits and applied `cross_val_score` to assess model robustness.

6. **Feature Importance**
   - Extracted feature importances from the trained Random Forest.
   - Plotted a bar chart to show the importance of each feature.

---

## 📝 Results:

- **Test Accuracy**: ~100% (varies slightly depending on random state)
- **Cross-validation mean accuracy**: ~96-98%
- **Important Features**:
  - Petal length and petal width contributed the most to predictions.

---

## 📌 Key Libraries Used:
- sklearn (datasets, model_selection, pipeline, ensemble, metrics, preprocessing)
- matplotlib, seaborn

---

## 📈 Visualization:
- Feature importance bar plot showing contribution of each feature toward model predictions.

---

## ✅ Learning Outcome:
This task demonstrated how to create a clean, scalable machine learning pipeline combining preprocessing, training, and evaluation. It also emphasized the importance of using cross-validation for generalization performance and visualizing model insights.
