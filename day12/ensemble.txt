# üìö Ensemble Learning on Multiple Datasets with Voting Classifier

## 1. ‚ú® What is Ensemble Learning?

Ensemble learning combines **multiple models** (like decision trees, linear models, etc.) to create a **stronger overall model**.  
The idea is that a group of models ("ensemble") will perform better than any individual model alone.

**Types of Ensemble Methods:**
- **Bagging (Bootstrap Aggregating)**: Reduces variance (example: Random Forest).
- **Boosting**: Reduces bias and variance (example: Gradient Boosting).
- **Voting**: Combines multiple models and predicts based on their "votes".

---

## 2. ‚ö° What is Voting Classifier?

- **Voting Classifier** is an ensemble technique that **combines predictions** from multiple models.
- There are two types:
  - **Hard Voting**: Majority voting (the class that gets most votes is chosen).
  - **Soft Voting**: Predicts the class label based on **averaged predicted probabilities**.

In this project, we used **Soft Voting** for better performance (especially useful when models output probabilities).

---

## 3. üìà Datasets Used

We applied the VotingClassifier on **three popular datasets** from scikit-learn:
- **Iris Dataset**: Classify types of flowers.
- **Wine Dataset**: Classify types of wines.
- **Breast Cancer Dataset**: Classify if tumors are malignant or benign.

---

## 4. üî® Models Used

- **Random Forest Classifier** (Bagging approach)
- **Gradient Boosting Classifier** (Boosting approach)

Both were combined inside a **VotingClassifier**.

---

## 5. üß© Workflow and Steps

1. Load datasets (Iris, Wine, Breast Cancer) using `load_iris()`, `load_wine()`, `load_breast_cancer()`.
2. Split datasets into **train and test sets** using `train_test_split()`.
3. Initialize **RandomForestClassifier** and **GradientBoostingClassifier**.
4. Create **separate VotingClassifier instances** for each dataset.
5. Train and predict on each dataset separately.
6. Evaluate using:
   - **Accuracy Score**
   - **Classification Report** (Precision, Recall, F1-score)
   - **Confusion Matrix**
7. Visualize Confusion Matrices with **Seaborn heatmaps**.

---

## 6. üíª Practical Code Summary

```python
# Import libraries
import pandas as pd
import numpy as np
from sklearn.datasets import load_iris, load_wine, load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Load datasets
iris = load_iris()
wine = load_wine()
breast_cancer = load_breast_cancer()

# Train-test split
x_train_iris, x_test_iris, y_train_iris, y_test_iris = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)
x_train_wine, x_test_wine, y_train_wine, y_test_wine = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)
x_train_breast_cancer, x_test_breast_cancer, y_train_breast_cancer, y_test_breast_cancer = train_test_split(
    breast_cancer.data, breast_cancer.target, test_size=0.2, random_state=42)

# Base models
rf = RandomForestClassifier(n_estimators=100, random_state=42)
gb = GradientBoostingClassifier(n_estimators=100, random_state=42)

# Voting classifiers
voting_iris = VotingClassifier(estimators=[('rf', rf), ('gb', gb)], voting='soft')
voting_wine = VotingClassifier(estimators=[('rf', rf), ('gb', gb)], voting='soft')
voting_breast_cancer = VotingClassifier(estimators=[('rf', rf), ('gb', gb)], voting='soft')

# Train models
voting_iris.fit(x_train_iris, y_train_iris)
voting_wine.fit(x_train_wine, y_train_wine)
voting_breast_cancer.fit(x_train_breast_cancer, y_train_breast_cancer)

# Predictions
y_pred_iris = voting_iris.predict(x_test_iris)
y_pred_wine = voting_wine.predict(x_test_wine)
y_pred_breast_cancer = voting_breast_cancer.predict(x_test_breast_cancer)

# Evaluation
def evaluate(y_true, y_pred, dataset_name):
    print(f"\n{dataset_name} Dataset")
    print(f"Accuracy: {accuracy_score(y_true, y_pred):.4f}")
    print("Classification Report:\n", classification_report(y_true, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred))

evaluate(y_test_iris, y_pred_iris, "Iris")
evaluate(y_test_wine, y_pred_wine, "Wine")
evaluate(y_test_breast_cancer, y_pred_breast_cancer, "Breast Cancer")

# Plot confusion matrices
for name, (y_true, y_pred) in {
    "Iris": (y_test_iris, y_pred_iris),
    "Wine": (y_test_wine, y_pred_wine),
    "Breast Cancer": (y_test_breast_cancer, y_pred_breast_cancer)
}.items():
    plt.figure(figsize=(8, 6))
    sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d', cmap='Blues')
    plt.title(f"Confusion Matrix for {name} Dataset")
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()
```

---

## 7. üèÜ Results (example outputs)

| Dataset         | Accuracy  |  
|:----------------|:----------|  
| Iris            | ~100%     |  
| Wine            | ~100%     |  
| Breast Cancer   | ~96%      |  

(Your results may vary slightly because of randomness in training.)

---

## 8. üìë Key Takeaways

- Ensemble learning significantly improves model performance.
- Soft Voting works better when models output **probability estimates**.
- Random Forest + Gradient Boosting together cover **both bagging and boosting strengths**.
- Always use **separate model instances** when training on different datasets.

