Theoretical Explanation: Overfitting, Underfitting & Bias-Variance Tradeoff

## **1. Fundamental Concepts**

### **Model Complexity Spectrum**
Models exist on a complexity continuum:
```
Underfitting (High Bias) ← Optimal Fit → Overfitting (High Variance)
```

### **Key Definitions**
- **Underfitting**: Model is too simple to capture data patterns
  - Symptoms: Poor performance on both training and test data
  - Cause: High bias in model assumptions

- **Overfitting**: Model memorizes training data including noise
  - Symptoms: Excellent training performance, poor generalization
  - Cause: High variance, excessive model flexibility

- **Bias-Variance Tradeoff**: Fundamental tension between:
  - **Bias**: Error from oversimplified assumptions
  - **Variance**: Error from excessive sensitivity to training data

## **2. Mathematical Formulation**

### **Expected Prediction Error Decomposition**
For any model, the test error can be decomposed as:

```
E[(y - ŷ)²] = Bias²(ŷ) + Var(ŷ) + Irreducible Error
```

Where:
- **Bias²**: Difference between expected prediction and true value
- **Variance**: Variability of model predictions
- **Irreducible Error**: Noise inherent in the data

### **Polynomial Regression Case**
For polynomial degree *d*:
- **Bias** ≈ ∑(ignored polynomial terms)  
- **Variance** ≈ O(d/n) (n = sample size)

## **3. Theoretical Analysis of Our Implementation**

### **Data Generation Process**
Our synthetic data follows:
```
y = sin(x₁) + 0.5x₂² + ε,  ε ~ N(0,σ²)
```
This represents:
- **Nonlinear** relationships (requires polynomial features)
- **Controlled noise** (σ = 0.3) for clear demonstration

### **Model Behavior by Degree**

| Degree | Theoretical Behavior | Practical Manifestation |
|--------|----------------------|-------------------------|
| 1 (Linear) | High bias: Cannot represent sin() or x² terms | Underfitting - poor accuracy |
| 3 | Balanced: Can approximate sin() via Taylor expansion | Good generalization |
| 10+ | High variance: Fits noise in training data | Overfitting - large gap between train/test accuracy |

## **4. Decision Boundary Theory**

The logistic regression decision boundary occurs at:
```
σ(w·Φ(x)) = 0.5 ⇒ w·Φ(x) = 0
```
Where Φ(x) are polynomial features. For degree *d*:
- **d=1**: Linear hyperplane (w₀ + w₁x₁ + w₂x₂ = 0)
- **d=3**: Quadratic surfaces with interaction terms
- **d=10**: Highly contoured, potentially disjoint regions

## **5. Bias-Variance Tradeoff Visualization**

Our accuracy plots demonstrate the U-shaped relationship:
```
Test Error
   ↑
   |     ____
   |    /
   |   /
   |__/
   +-----------→
     Model Complexity
```

Key points:
1. Left region: Dominated by bias
2. Minimum: Optimal complexity
3. Right region: Dominated by variance

## **6. Practical Implications**

### **Model Selection Guidelines**
1. **When underfitting is detected** (high bias):
   - Increase model capacity
   - Add relevant features
   - Use more complex algorithms

2. **When overfitting is detected** (high variance):
   - Reduce model complexity
   - Apply regularization (L1/L2)
   - Increase training data
   - Use feature selection

### **Regularization Perspective**
Our implementation uses unregularized logistic regression. Adding L2 regularization would modify the loss function:
```
J(w) = Cross-Entropy + λ||w||²
```
Where λ controls the bias-variance tradeoff:
- Large λ → Higher bias, lower variance
- Small λ → Lower bias, higher variance

## **7. Extension to Other Models**

While demonstrated with polynomial regression, these concepts generalize:

| Model Type | Complexity Control | Bias-Variance Knobs |
|------------|--------------------|---------------------|
| Decision Trees | max_depth | Pruning, min_samples_leaf |
| Neural Networks | Hidden units/layers | Dropout, Weight decay |
| SVM | Kernel choice | C parameter, γ in RBF |

## **8. Advanced Considerations**

### **Double Descent Phenomenon**
In modern overparameterized models (e.g., deep learning), the classical U-curve may give way to:
```
Test Error
   ↑
   |\    /
   | \__/
   +-----------→
     Model Complexity
```
Where increasing complexity beyond interpolation threshold can sometimes improve performance.

### **VC Dimension Theory**
The Vapnik-Chervonenkis dimension provides a theoretical framework for model capacity:
- Higher VC dimension → greater risk of overfitting
- Related to number of parameters but not identical


This demonstration provides a concrete implementation of fundamental statistical learning theory concepts. The polynomial logistic regression serves as an excellent pedagogical tool because:
1. Model complexity is clearly controlled via degree
2. Decision boundaries are visually interpretable
3. The bias-variance tradeoff manifests clearly
